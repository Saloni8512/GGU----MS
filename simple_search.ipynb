{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified data structure for Term search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to pip install tika\n",
    "from tika import parser\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change file path\n",
    "\n",
    "k = parser.from_file('c:\\\\users\\\\hello\\\\desktop\\\\metric_stream\\\\v0.02\\\\data\\\\Converted contract - Acciona (Spain) ASP Executed.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_doc = k['content']\n",
    "#print(pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library of terminology\n",
    "\n",
    "category_term1 = \\\n",
    "    {'Time': ['hours', 'days', 'date', 'term', 'termination'], \\\n",
    "    'Person': ['owner', 'client', 'contact', 'name'], \\\n",
    "    'Agreement': ['agreement', 'renewal', 'backup', 'breach', 'churn', 'user', 'support', 'uptime'],\\\n",
    "    'Financial': ['value', 'fee', 'currency', 'escrow', 'renewal', 'term', 'price'],\\\n",
    "    'Address': ['address'], \\\n",
    "    'Product': ['SKU', 'license', 'user'], \\\n",
    "    'Legal': ['contract', 'legal', 'law', 'liability', 'indemnity', 'term', 'penalty', 'renegotiation'],\\\n",
    "    'Financial': ['fee']}\n",
    "\n",
    "term1_term2 = \\\n",
    "    {'hours': ['12', '24', '48'],\\\n",
    "     'days': ['15', '30'],\\\n",
    "     'owner': ['account', 'opportunity'],\\\n",
    "     'agreement': ['type'],\\\n",
    "     'value': ['renewal'],\\\n",
    "     'fee': ['automatically','cloud','license','partner','platform','support','product'],\\\n",
    "     'renewal': ['automatically', 'date'],\\\n",
    "     'address': ['billing', 'corporate'],\\\n",
    "     'churn': ['acv', 'type'],\\\n",
    "     'date': ['churn','effective','executed','renewal','notice','subscription','term'],\\\n",
    "     'client': ['consent', 'name'],\\\n",
    "     'SKU': ['cloud', 'product'],\\\n",
    "     'contract': ['term', 'relationship'],\\\n",
    "     'legal': ['customer', 'contact'],\\\n",
    "     'law': ['governing'],\\\n",
    "     'license': ['type'],\\\n",
    "     'user': ['license', 'maximum', 'total'],\\\n",
    "     'liability': ['limitation'],\\\n",
    "     'contact': ['operation'],\\\n",
    "     'indemnity': ['nonstandard'],\\\n",
    "     'term': ['order', 'payment', 'renewal'],\\\n",
    "     'name': ['partner'],\\\n",
    "     'penalty': ['sla'],\\\n",
    "     'renegotiation': ['comments'],\\\n",
    "     'price': ['renewal'],\\\n",
    "     'termination': ['convenience'],\\\n",
    "     'support': ['type']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler \n",
    "\n",
    "def sent_segment(txt):\n",
    "    \"\"\" sentence tokenization\n",
    "\n",
    "    Parameters:\n",
    "    txt : tex to tokenize into sentences\n",
    "    Returns: list of sentences\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = English() \n",
    "\n",
    "    # A simple pipeline component, to allow custom sentence boundary detection logic \n",
    "    # that doesnâ€™t require the dependency parse. It splits on punctuation by default\n",
    "    sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "    # Add the component to the pipeline\n",
    "    nlp.add_pipe(sbd)\n",
    "\n",
    "    #nlp is used to create documents with linguistic annotations.\n",
    "    doc = nlp(txt)   \n",
    "\n",
    "    # create list of sentence tokens\n",
    "    sents_list = []\n",
    "    for sent in doc.sents:\n",
    "        sents_list.append(sent.text)\n",
    "\n",
    "    return sents_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonym finder\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "\n",
    "# #find synonyms based on the keyword and build into a dataframe\n",
    "def get_synonyms(term_dic):\n",
    "\n",
    "    #list of tuples containing (category, original term, synonym)\n",
    "    term_tuples = []\n",
    "\n",
    "    \n",
    "    ####Issues passing number into wordnet. \n",
    "    L = []\n",
    "    for key in term_dic:\n",
    "        for item in term_dic[key]:\n",
    "            syn = wordnet.synsets(item)\n",
    "            \n",
    "            syn_list = [item]\n",
    "\n",
    "            #flatten all lists by chain, remove duplicates by set\n",
    "            lemmas = list(set(chain.from_iterable([w.lemma_names() for w in syn])))\n",
    "            \n",
    "            for i in lemmas[:3]:\n",
    "                syn_list.append(i)\n",
    "    \n",
    "            syn_list = list(set(syn_list))\n",
    "        \n",
    "            for syn in syn_list:\n",
    "                L.append((key, item, syn))\n",
    "\n",
    "            \n",
    "    return (pd.DataFrame(L, columns=['Category','Original_Keyword','Searched_Keyword']))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_ex = {'fine': ['fee']}\n",
    "# get_synonyms(dic_ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search proper keyword in the list of sentences\n",
    "# if found, grab surrounding lines, combine, and add to 3rd dataframe\n",
    "\n",
    "#sentence_df contains list of sentence segment in each row\n",
    "#term_df contains the category, original term, and synonym term in each row\n",
    "# Want to search the synonym term in sentence rows. Then if match found, add all those 4 columns to new dataframe\n",
    "\n",
    "\n",
    "def search_term(sentence_df, term_df):\n",
    "    \n",
    "    category_column_names = ['Category', 'Original_Keyword', 'Searched_Keyword', 'sentence_number','Text']\n",
    "    df_metadata = pd.DataFrame(columns = category_column_names)\n",
    "\n",
    "    array_metadata = df_metadata.values\n",
    "    \n",
    "    for i,j in sentence_df.iterrows():\n",
    "        for m,n in term_df.iterrows():\n",
    "            if n['Searched_Keyword'] in j['Sentences'].lower():\n",
    "                ####### do the iteration, search the term and add to dataframe\n",
    "                \n",
    "                df_found = pd.DataFrame([(n['Category'], n['Original_Keyword'], n['Searched_Keyword'], i, j['Sentences'])])\n",
    "                \n",
    "                array_metadata = np.concatenate(\\\n",
    "                        (array_metadata, df_found.values), axis=0)\n",
    "\n",
    "    df_metadata = pd.DataFrame(array_metadata, columns = category_column_names)\n",
    "    \n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "#currently adding things to an np.array continually. then at the end, add it to the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create context. takes current line of text and associated index. \n",
    "# lokos at the sentence dataframe and grabs correct rows.\n",
    "# allows us to search multiple segments for each original term\n",
    "\n",
    "#pass the original sentence dataframe and current row of the metadata dataframe\n",
    "def create_context(sentence_df, metadata_df, idx):\n",
    "    \n",
    "    sentence_idx = metadata_df.iloc[idx]['sentence_number']\n",
    "    \n",
    "    try:\n",
    "        context = sentence_df.iloc[sentence_idx - 1]['Sentences'] + \\\n",
    "        sentence_df.iloc[sentence_idx]['Sentences'] + \\\n",
    "        sentence_df.iloc[sentence_idx + 1]['Sentences']\n",
    "    except KeyError:\n",
    "        \n",
    "        context = sentence_df.iloc[sentence_idx]['Sentences']\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses a secondary library of terms\n",
    "# Keys in this library will match the 'original keyword' column in the metadata_df\n",
    "# Iterate through associated list of terms\n",
    "# If secondary term found, add it to new column\n",
    "# adds to an existing dataframe with enough columns so just need to generate list and add to new column\n",
    "\n",
    "#Need to add greater context to each line. grab sentence from sentence dataframe\n",
    "\n",
    "def search_secondary(sentence_df, metadata_df, term_dic):\n",
    "    \n",
    "    secondary_term_outer = []\n",
    "    secondary_syn_outer = []\n",
    "    secondary_context_outer = []\n",
    "    \n",
    "    for i,j in metadata_df.iterrows():\n",
    "        \n",
    "        context = create_context(sentence_df, metadata_df, i)\n",
    "        \n",
    "        secondary_term_inner = [] \n",
    "        secondary_syn_inner = []\n",
    "        secondary_context_inner = None\n",
    "        \n",
    "        for key in term_dic:\n",
    "            \n",
    "            for item in term_dic[key]:\n",
    "\n",
    "                syn = wordnet.synsets(item)\n",
    "                \n",
    "                syn_list = [item]\n",
    "                \n",
    "                #flatten all lists by chain, remove duplicates by set\n",
    "                lemmas = list(set(chain.from_iterable([w.lemma_names() for w in syn])))                \n",
    "                \n",
    "                for m in lemmas[:3]:\n",
    "                    syn_list.append(m)\n",
    "\n",
    "                syn_list = list(set(syn_list))\n",
    "\n",
    "                \n",
    "                for syn in syn_list:\n",
    "\n",
    "                    if syn in context.lower() and key == j['Original_Keyword'] and secondary_context_inner == None:\n",
    "                        secondary_term_inner.append(item)\n",
    "                        secondary_syn_inner.append(syn)\n",
    "                        secondary_context_inner = context\n",
    "\n",
    "                    elif syn in context.lower() and key == j['Original_Keyword']:\n",
    "                        secondary_term_inner.append(item)\n",
    "                        secondary_syn_inner.append(syn)\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                \n",
    "        secondary_term_outer.append(secondary_term_inner)\n",
    "        secondary_syn_outer.append(secondary_syn_inner)\n",
    "        secondary_context_outer.append(secondary_context_inner)\n",
    "    \n",
    "    metadata_df['Secondary_Term'] = secondary_term_outer\n",
    "    metadata_df['Secondary_Synonym'] = secondary_syn_outer\n",
    "    metadata_df['Secondary_Context'] = secondary_context_outer\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "Start with input dictionary containing category and keyword.  \n",
    "Use keyword to search synonyms. Create a tuple of (category, keyword, synonym) and append those to a list.  \n",
    "Iterate through list and search for synonym in text of df_sentence.  \n",
    "Create a dataframe of ['category', 'keyword', 'synonym', 'text']  \n",
    "\n",
    "\n",
    "Best algorithm for this? Right now looking at O(n_squared)\n",
    "    - (number of terms) * (number of sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Europa , 18 P.E. La Moraleja 28108 Alcobendas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>â€œOrder Form\" Attached as Exhibit A â€œService Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>This Agreement shall consist of the Metric Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>This Agreement constitutes the entire agreemen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences\n",
       "0   DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...\n",
       "1  Europa , 18 P.E. La Moraleja 28108 Alcobendas ...\n",
       "2  â€œOrder Form\" Attached as Exhibit A â€œService Le...\n",
       "3  This Agreement shall consist of the Metric Str...\n",
       "4  This Agreement constitutes the entire agreemen..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = sent_segment(pdf_doc)\n",
    "sentence_list_clean = []\n",
    "\n",
    "for i in range(0,len(sentence_list)):\n",
    "    remove_newline = sentence_list[i].replace('\\n', '')\n",
    "    sentence_list_clean.append(remove_newline)\n",
    "    \n",
    "#Write sentence list dataframe to file\n",
    "\n",
    "dic_sentence = {'Sentences' : sentence_list_clean} \n",
    "df_sentence = pd.DataFrame.from_dict(dic_sentence)\n",
    "df_sentence.head()\n",
    "\n",
    "#df_sentence.to_csv('sentences0604.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_metadata0.to_csv('metadata_test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = get_synonyms(category_term1)\n",
    "df_metadata0 = search_term(df_sentence, df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Original_Keyword</th>\n",
       "      <th>Searched_Keyword</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Time</td>\n",
       "      <td>date</td>\n",
       "      <td>date</td>\n",
       "      <td>0</td>\n",
       "      <td>DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Agreement</td>\n",
       "      <td>agreement</td>\n",
       "      <td>agreement</td>\n",
       "      <td>0</td>\n",
       "      <td>DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Legal</td>\n",
       "      <td>law</td>\n",
       "      <td>law</td>\n",
       "      <td>0</td>\n",
       "      <td>DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Time</td>\n",
       "      <td>term</td>\n",
       "      <td>condition</td>\n",
       "      <td>1</td>\n",
       "      <td>Europa , 18 P.E. La Moraleja 28108 Alcobendas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Time</td>\n",
       "      <td>term</td>\n",
       "      <td>term</td>\n",
       "      <td>1</td>\n",
       "      <td>Europa , 18 P.E. La Moraleja 28108 Alcobendas ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category Original_Keyword Searched_Keyword sentence_number  \\\n",
       "0       Time             date             date               0   \n",
       "1  Agreement        agreement        agreement               0   \n",
       "2      Legal              law              law               0   \n",
       "3       Time             term        condition               1   \n",
       "4       Time             term             term               1   \n",
       "\n",
       "                                                Text  \n",
       "0   DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...  \n",
       "1   DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...  \n",
       "2   DocuSign Envelope ID: F3F9AB55-4121-49E9-BE97...  \n",
       "3  Europa , 18 P.E. La Moraleja 28108 Alcobendas ...  \n",
       "4  Europa , 18 P.E. La Moraleja 28108 Alcobendas ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Original_Keyword</th>\n",
       "      <th>Searched_Keyword</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>Agreement</td>\n",
       "      <td>backup</td>\n",
       "      <td>support</td>\n",
       "      <td>277</td>\n",
       "      <td>MetricStream will not store Customer data on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>Agreement</td>\n",
       "      <td>support</td>\n",
       "      <td>support</td>\n",
       "      <td>277</td>\n",
       "      <td>MetricStream will not store Customer data on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>Time</td>\n",
       "      <td>term</td>\n",
       "      <td>term</td>\n",
       "      <td>278</td>\n",
       "      <td>Metric Stream will not store customer data for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>Agreement</td>\n",
       "      <td>agreement</td>\n",
       "      <td>agreement</td>\n",
       "      <td>278</td>\n",
       "      <td>Metric Stream will not store customer data for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>Legal</td>\n",
       "      <td>term</td>\n",
       "      <td>term</td>\n",
       "      <td>278</td>\n",
       "      <td>Metric Stream will not store customer data for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category Original_Keyword Searched_Keyword sentence_number  \\\n",
       "444  Agreement           backup          support             277   \n",
       "445  Agreement          support          support             277   \n",
       "446       Time             term             term             278   \n",
       "447  Agreement        agreement        agreement             278   \n",
       "448      Legal             term             term             278   \n",
       "\n",
       "                                                  Text  \n",
       "444  MetricStream will not store Customer data on a...  \n",
       "445  MetricStream will not store Customer data on a...  \n",
       "446  Metric Stream will not store customer data for...  \n",
       "447  Metric Stream will not store customer data for...  \n",
       "448  Metric Stream will not store customer data for...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata0.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_2 = search_secondary(df_sentence, df_metadata0, term1_term2)\n",
    "df_metadata_2.to_csv(\"secondary_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
