{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to pip install tika\n",
    "from tika import parser\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metadata Term Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return true if our current value is NaN\n",
    "\n",
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "#generate dictionary \n",
    "#key is metadata field\n",
    "#value is a list of term1, term2, term3\n",
    "# we use this to match metadata fields to found terms in our sentence output document\n",
    "\n",
    "def metadata_term_library(df_metadata):\n",
    "    \n",
    "    dic_metadata_terms = {}\n",
    "    dic_metadata_category = {}\n",
    "    \n",
    "    for i,j in df_metadata.iterrows():\n",
    "        \n",
    "        dic_metadata_category[j['Metadata']] = [j['Metadata_Category']]\n",
    "        dic_metadata_terms[j['Metadata']] = [j['Term1']]\n",
    "                \n",
    "        if not isNan(j['Term2']):\n",
    "            dic_metadata_terms[j['Metadata']].append(j['Term2'])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if not isNan(j['Term3']):\n",
    "            dic_metadata_terms[j['Metadata']].append(j['Term3'])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return(dic_metadata_terms, dic_metadata_category)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified data structure for Term search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler \n",
    "\n",
    "def sent_segment(txt):\n",
    "    \"\"\" sentence tokenization\n",
    "\n",
    "    Parameters:\n",
    "    txt : tex to tokenize into sentences\n",
    "    Returns: list of sentences\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = English() \n",
    "\n",
    "    # A simple pipeline component, to allow custom sentence boundary detection logic \n",
    "    # that doesn’t require the dependency parse. It splits on punctuation by default\n",
    "    sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "    # Add the component to the pipeline\n",
    "    nlp.add_pipe(sbd)\n",
    "\n",
    "    #nlp is used to create documents with linguistic annotations.\n",
    "    doc = nlp(txt)   \n",
    "\n",
    "    # create list of sentence tokens\n",
    "    sents_list = []\n",
    "    for sent in doc.sents:\n",
    "        sents_list.append(sent.text)\n",
    "\n",
    "    return sents_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonym finder\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "\n",
    "# #find synonyms based on the keyword and build into a dataframe\n",
    "def get_synonyms(term_dic):\n",
    "\n",
    "    #list of tuples containing (category, original term, synonym)\n",
    "    term_tuples = []\n",
    "\n",
    "    \n",
    "    ####Issues passing number into wordnet. \n",
    "    L = []\n",
    "    for key in term_dic:\n",
    "\n",
    "        syn = wordnet.synsets(str(term_dic[key][0]))\n",
    "\n",
    "            \n",
    "        syn_list = [term_dic[key][0]]\n",
    "\n",
    "        #flatten all lists by chain, remove duplicates by set\n",
    "        lemmas = list(set(chain.from_iterable([w.lemma_names() for w in syn])))\n",
    "\n",
    "        for i in lemmas[:3]:\n",
    "            syn_list.append(i)\n",
    "\n",
    "        syn_list = list(set(syn_list))\n",
    "\n",
    "        for syn in syn_list:\n",
    "            L.append((key, term_dic[key][0], syn))\n",
    "\n",
    "            \n",
    "    return (pd.DataFrame(L, columns=['Metadata_Field','Original_Term','Synonym_Term']))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataframe containing Metadata field, original term1, and the synonym term1\n",
    "\n",
    "# df_term1 = get_synonyms(dic_meta_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search proper keyword in the list of sentences\n",
    "# if found, grab surrounding lines, combine, and add to 3rd dataframe\n",
    "\n",
    "#sentence_df contains list of sentence segment in each row\n",
    "#term_df contains the category, original term, and synonym term in each row\n",
    "# Want to search the synonym term in sentence rows. Then if match found, add all those 4 columns to new dataframe\n",
    "\n",
    "\n",
    "def search_term(sentence_df, term_df):\n",
    "    \n",
    "    category_column_names = ['Metadata_Field','Original_Keyword', 'Searched_Keyword', 'Sentence_Number','Text']\n",
    "    df_metadata = pd.DataFrame(columns = category_column_names)\n",
    "\n",
    "    array_metadata = df_metadata.values\n",
    "    \n",
    "    for i,j in sentence_df.iterrows():\n",
    "        for m,n in term_df.iterrows():\n",
    "            if str(n['Synonym_Term']) in j['Sentences'].lower():\n",
    "                ####### do the iteration, search the term and add to dataframe\n",
    "                \n",
    "                df_found = pd.DataFrame([(n['Metadata_Field'],n['Original_Term'], n['Synonym_Term'], i, j['Sentences'])])\n",
    "                \n",
    "                array_metadata = np.concatenate(\\\n",
    "                        (array_metadata, df_found.values), axis=0)\n",
    "\n",
    "    df_metadata = pd.DataFrame(array_metadata, columns = category_column_names)\n",
    "    \n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "#currently adding things to an np.array continually. then at the end, add it to the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create context. takes current line of text and associated index. \n",
    "# lokos at the sentence dataframe and grabs correct rows.\n",
    "# allows us to search multiple segments for each original term\n",
    "\n",
    "#pass the original sentence dataframe and current row of the metadata dataframe\n",
    "def create_context(sentence_df, metadata_df, idx):\n",
    "    \n",
    "    sentence_idx = metadata_df.iloc[idx]['Sentence_Number']\n",
    "    \n",
    "    try:\n",
    "        context = sentence_df.iloc[sentence_idx - 1]['Sentences'] + \\\n",
    "        sentence_df.iloc[sentence_idx]['Sentences'] + \\\n",
    "        sentence_df.iloc[sentence_idx + 1]['Sentences']\n",
    "    except:\n",
    "        \n",
    "        context = sentence_df.iloc[sentence_idx]['Sentences']\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses a secondary library of terms\n",
    "# Keys in this library will match the 'original keyword' column in the metadata_df\n",
    "# Iterate through associated list of terms\n",
    "# If secondary term found, add it to new column\n",
    "# adds to an existing dataframe with enough columns so just need to generate list and add to new column\n",
    "\n",
    "#Need to add greater context to each line. grab sentence from sentence dataframe\n",
    "\n",
    "def search_secondary(sentence_df, metadata_df, term_dic):\n",
    "    \n",
    "    secondary_term_outer = []\n",
    "    secondary_syn_outer = []\n",
    "    secondary_context_outer = []\n",
    "    \n",
    "    for i,j in metadata_df.iterrows():\n",
    "        \n",
    "        context = create_context(sentence_df, metadata_df, i)\n",
    "        \n",
    "        secondary_term_inner = [] \n",
    "        secondary_syn_inner = []\n",
    "        secondary_context_inner = None\n",
    "        term1 = term_dic[j['Metadata_Field']]\n",
    "        \n",
    "    \n",
    "        if len(term1) > 1:\n",
    "\n",
    "            for k in range(1,len(term1)):\n",
    "\n",
    "                item = str(term1[k])\n",
    "\n",
    "                syn = wordnet.synsets(item)\n",
    "\n",
    "                syn_list = [item]\n",
    "\n",
    "                #flatten all lists by chain, remove duplicates by set\n",
    "                lemmas = list(set(chain.from_iterable([w.lemma_names() for w in syn])))                \n",
    "\n",
    "                for m in lemmas[:3]:\n",
    "                    syn_list.append(m)\n",
    "\n",
    "                syn_list = list(set(syn_list))\n",
    "\n",
    "                for syn in syn_list:\n",
    "\n",
    "                    if syn in context.lower() and secondary_context_inner == None:\n",
    "                        secondary_term_inner.append(item)\n",
    "                        secondary_syn_inner.append(syn)\n",
    "                        secondary_context_inner = context\n",
    "\n",
    "                    elif syn in context.lower():\n",
    "                        secondary_term_inner.append(item)\n",
    "                        secondary_syn_inner.append(syn)\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "                                      \n",
    "        secondary_term_outer.append(secondary_term_inner)\n",
    "        secondary_syn_outer.append(secondary_syn_inner)\n",
    "        secondary_context_outer.append(secondary_context_inner)\n",
    "    \n",
    "    metadata_df['Secondary_Term'] = secondary_term_outer\n",
    "    metadata_df['Secondary_Synonym'] = secondary_syn_outer\n",
    "    metadata_df['Secondary_Context'] = secondary_context_outer\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs metadata dictionary\n",
    "#sentence found keyword dataframe\n",
    "#iterate through sentence dataframe\n",
    "# look at the combined terminology list of original_keyword and seondary_term\n",
    "# see if metadata search temrs are contained in the list\n",
    "# if all terms are in the sentence term list, then add the associated metadata term to the row as possible match\n",
    "\n",
    "def combine_keyword_list(df_keyword):\n",
    "    \n",
    "    complete_term_list = []\n",
    "    \n",
    "    for i,j in df_keyword.iterrows():\n",
    "        secondary_term_tolist = eval(j['Secondary_Term'])\n",
    "        original_keyword_string = str(j['Original_Keyword'])\n",
    "        \n",
    "        #print(type(original_keyword_string))\n",
    "        \n",
    "        #print(secondary_term_tolist)\n",
    "        secondary_term_tolist.append(original_keyword_string)\n",
    "        complete_terms = secondary_term_tolist\n",
    "        \n",
    "        complete_term_list.append(complete_terms)\n",
    "        \n",
    "    df_keyword['Complete_Term_List'] = complete_term_list\n",
    "    \n",
    "    return df_keyword\n",
    "\n",
    "##test\n",
    "#combine_keyword_list(df_sentence_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check each row to see if metadata fields are in the keywords list\n",
    "# create new column in dataframe that shows all possible metadata matches\n",
    "\n",
    "def metadata_match(df_keyword, dic_metadata, dic_category):\n",
    "    \n",
    "    metadata_matches_outer = []\n",
    "    category1_match_outer = []\n",
    "    category2_match_outer = []\n",
    "    \n",
    "    for i,j in df_keyword.iterrows():\n",
    "        \n",
    "        metadata_matches_inner = []\n",
    "        category1_match_inner = []\n",
    "        category2_match_inner = []\n",
    "                   \n",
    "        term_list = j['Complete_Term_List']\n",
    "        \n",
    "        for key in dic_metadata:\n",
    "            \n",
    "            if all(elem in term_list for elem in dic_metadata[key]):\n",
    "                metadata_matches_inner.append(key)\n",
    "                category1_match_inner.append(dic_category[key][0])\n",
    "                   \n",
    "                try: \n",
    "                    category2_match_inner.append(dic_category[key][1])\n",
    "                except:\n",
    "                    category2_match_inner.append(None)\n",
    "                   \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        metadata_matches_outer.append(metadata_matches_inner)\n",
    "        category1_match_outer.append(list(set(category1_match_inner)))\n",
    "        category2_match_outer.append(list(set(category2_match_inner)))   \n",
    "    \n",
    "    df_keyword['Metadata_Matches'] = metadata_matches_outer\n",
    "    df_keyword['Metadata_Category'] = category1_match_outer\n",
    "    df_keyword['Metadata_SubCategory'] = category2_match_outer\n",
    "    \n",
    "    #remove lines with know metadata field match\n",
    "    df_keyword = df_keyword[df_keyword['Metadata_Matches'].map(lambda d: len(d)) > 0]\n",
    "    \n",
    "    #print(df_keyword['Metadata_Matches'][1])\n",
    "    return df_keyword\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "Start with input dictionary containing category and keyword.  \n",
    "Use keyword to search synonyms. Create a tuple of (category, keyword, synonym) and append those to a list.  \n",
    "Iterate through list and search for synonym in text of df_sentence.  \n",
    "Create a dataframe of ['category', 'keyword', 'synonym', 'text']  \n",
    "\n",
    "\n",
    "Best algorithm for this? Right now looking at O(n_squared)\n",
    "    - (number of terms) * (number of sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-11 16:36:46,777 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#create contract list\n",
    "contract_path = 'C:\\\\Users\\\\Hello\\\\Desktop\\\\metric_stream\\\\v0.022\\\\metadata_pipeline\\\\data\\\\'\n",
    "#contract_names = listdir(contract_path)\n",
    "#remove .pdf Use for file name in csv export at end\n",
    "\n",
    "contract_files = [contract_path + f for f in listdir(contract_path) if isfile(join(contract_path, f))]\n",
    "\n",
    "#can loop through list for contract files\n",
    "parsed_contract = parser.from_file(contract_files[0])\n",
    "pdf_doc = parsed_contract['content']\n",
    "\n",
    "\n",
    "#print(contract_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_meta_term is list of terminology associated with metadata field\n",
    "#dic_meta_category is mapping of metadata field to a metadata category (original metadata field provided to us)\n",
    "metadata_file = pd.read_excel('metadata_library.xlsx')\n",
    "dic_meta_term, dic_meta_category = metadata_term_library(metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MetricStream  1 | P a g e      MS-INF-CLD-PRM/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3 This Order Form (“Order Form”) supplements t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MetricStream and Customer may be referred to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>For good and valuable consideration, the recei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Type of License:   Annual Subscription License.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences\n",
       "0  MetricStream  1 | P a g e      MS-INF-CLD-PRM/...\n",
       "1  3 This Order Form (“Order Form”) supplements t...\n",
       "2  MetricStream and Customer may be referred to i...\n",
       "3  For good and valuable consideration, the recei...\n",
       "4    Type of License:   Annual Subscription License."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = sent_segment(pdf_doc)\n",
    "sentence_list_clean = []\n",
    "\n",
    "for i in range(0,len(sentence_list)):\n",
    "    remove_newline = sentence_list[i].replace('\\n', '')\n",
    "    sentence_list_clean.append(remove_newline)\n",
    "    \n",
    "#Write sentence list dataframe to file\n",
    "\n",
    "dic_sentence = {'Sentences' : sentence_list_clean} \n",
    "df_sentence = pd.DataFrame.from_dict(dic_sentence)\n",
    "df_sentence.head()\n",
    "\n",
    "#df_sentence.to_csv('sentences0604.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match sentences with Metadata terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe containing Metadata field, original term1, and the synonym term1\n",
    "df_words = get_synonyms(dic_meta_term)\n",
    "\n",
    "\n",
    "df_metadata0 = search_term(df_sentence, df_words)\n",
    "\n",
    "df_metadata_2 = search_secondary(df_sentence, df_metadata0, dic_meta_term)\n",
    "#df_metadata_2.to_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match found metadata terms to original metadata field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_terms = combine_keyword_list(df_metadata_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined_terms generated above\n",
    "#AAAAAA associates the combined term list to a metadata field\n",
    "#BBBBBBB associates the metadata field with Group and Sub-group\n",
    "\n",
    "#metadata_match(df_combined_terms, AAAAAAA,BBBBBBBB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
