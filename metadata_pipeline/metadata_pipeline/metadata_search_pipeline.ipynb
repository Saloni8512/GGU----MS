{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to pip install tika\n",
    "from tika import parser\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metadata Term Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return true if our current value is NaN\n",
    "\n",
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "#generate dictionary \n",
    "#key is metadata field\n",
    "#value is a list of term1, term2, term3\n",
    "# we use this to match metadata fields to found terms in our sentence output document\n",
    "\n",
    "def metadata_term_library(df_metadata):\n",
    "    \n",
    "    dic_metadata_terms = {}\n",
    "    dic_metadata_original = {}\n",
    "    dic_metadata_category = {}\n",
    "    dic_metadata_subcategory = {}\n",
    "    \n",
    "    for i,j in df_metadata.iterrows():\n",
    "        \n",
    "        dic_metadata_original[j['Metadata']] = [j['Metadata_Original']]\n",
    "        dic_metadata_category[j['Metadata']] = [j['Category']]\n",
    "        dic_metadata_subcategory[j['Metadata']] = [j['Sub_Category']]\n",
    "\n",
    "        dic_metadata_terms[j['Metadata']] = [j['Term1']]\n",
    "                \n",
    "        if not isNan(j['Term2']):\n",
    "            dic_metadata_terms[j['Metadata']].append(j['Term2'])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if not isNan(j['Term3']):\n",
    "            dic_metadata_terms[j['Metadata']].append(j['Term3'])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return(dic_metadata_terms, dic_metadata_original, dic_metadata_category, dic_metadata_subcategory)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified data structure for Term search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler \n",
    "\n",
    "def sent_segment(txt):\n",
    "    \"\"\" sentence tokenization\n",
    "\n",
    "    Parameters:\n",
    "    txt : tex to tokenize into sentences\n",
    "    Returns: list of sentences\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = English() \n",
    "\n",
    "    # A simple pipeline component, to allow custom sentence boundary detection logic \n",
    "    # that doesn’t require the dependency parse. It splits on punctuation by default\n",
    "    sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "    # Add the component to the pipeline\n",
    "    nlp.add_pipe(sbd)\n",
    "\n",
    "    #nlp is used to create documents with linguistic annotations.\n",
    "    doc = nlp(txt)   \n",
    "\n",
    "    # create list of sentence tokens\n",
    "    sents_list = []\n",
    "    for sent in doc.sents:\n",
    "        sents_list.append(sent.text)\n",
    "\n",
    "    return sents_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonym finder\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "\n",
    "# #find synonyms based on the keyword and build into a dataframe\n",
    "def get_synonyms(term_dic):\n",
    "\n",
    "    #list of tuples containing (category, original term, synonym)\n",
    "    term_tuples = []\n",
    "\n",
    "    \n",
    "    ####Issues passing number into wordnet. \n",
    "    L = []\n",
    "    for key in term_dic:\n",
    "\n",
    "        syn = wordnet.synsets(str(term_dic[key][0]))\n",
    "\n",
    "            \n",
    "        syn_list = [term_dic[key][0]]\n",
    "\n",
    "        #flatten all lists by chain, remove duplicates by set\n",
    "        lemmas = list(set(chain.from_iterable([w.lemma_names() for w in syn])))\n",
    "\n",
    "        for i in lemmas[:3]:\n",
    "            syn_list.append(i)\n",
    "\n",
    "        syn_list = list(set(syn_list))\n",
    "\n",
    "        for syn in syn_list:\n",
    "            L.append((key, term_dic[key][0], syn))\n",
    "\n",
    "            \n",
    "    return (pd.DataFrame(L, columns=['Metadata_Field','Original_Term','Synonym_Term']))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataframe containing Metadata field, original term1, and the synonym term1\n",
    "\n",
    "# df_term1 = get_synonyms(dic_meta_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search proper keyword in the list of sentences\n",
    "# if found, grab surrounding lines, combine, and add to 3rd dataframe\n",
    "\n",
    "#sentence_df contains list of sentence segment in each row\n",
    "#term_df contains the category, original term, and synonym term in each row\n",
    "# Want to search the synonym term in sentence rows. Then if match found, add all those 4 columns to new dataframe\n",
    "\n",
    "\n",
    "def search_term(sentence_df, term_df, doc_name):\n",
    "    \n",
    "    category_column_names = ['Document Name','Metadata_Field','Original_Keyword', 'Searched_Keyword', 'Sentence_Number','Text']\n",
    "    df_metadata = pd.DataFrame(columns = category_column_names)\n",
    "\n",
    "    array_metadata = df_metadata.values\n",
    "    \n",
    "    for i,j in sentence_df.iterrows():\n",
    "        for m,n in term_df.iterrows():\n",
    "            if str(n['Synonym_Term']) in j['Sentences'].lower():\n",
    "                ####### do the iteration, search the term and add to dataframe\n",
    "                \n",
    "                df_found = pd.DataFrame([(doc_name, n['Metadata_Field'],n['Original_Term'], n['Synonym_Term'], i, j['Sentences'])])\n",
    "                \n",
    "                array_metadata = np.concatenate(\\\n",
    "                        (array_metadata, df_found.values), axis=0)\n",
    "\n",
    "    df_metadata = pd.DataFrame(array_metadata, columns = category_column_names)\n",
    "    \n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "#currently adding things to an np.array continually. then at the end, add it to the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create context. takes current line of text and associated index. \n",
    "# lokos at the sentence dataframe and grabs correct rows.\n",
    "# allows us to search multiple segments for each original term\n",
    "\n",
    "#pass the original sentence dataframe and current row of the metadata dataframe\n",
    "def create_context(sentence_df, metadata_df, idx):\n",
    "    \n",
    "    sentence_idx = metadata_df.iloc[idx]['Sentence_Number']\n",
    "    \n",
    "    try:\n",
    "        context = sentence_df.iloc[sentence_idx - 1]['Sentences'] + \\\n",
    "        sentence_df.iloc[sentence_idx]['Sentences'] + \\\n",
    "        sentence_df.iloc[sentence_idx + 1]['Sentences']\n",
    "    except:\n",
    "        \n",
    "        context = sentence_df.iloc[sentence_idx]['Sentences']\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses a secondary library of terms\n",
    "# Keys in this library will match the 'original keyword' column in the metadata_df\n",
    "# Iterate through associated list of terms\n",
    "# If secondary term found, add it to new column\n",
    "# adds to an existing dataframe with enough columns so just need to generate list and add to new column\n",
    "\n",
    "#Need to add greater context to each line. grab sentence from sentence dataframe\n",
    "\n",
    "def search_secondary(sentence_df, metadata_df, term_dic):\n",
    "    \n",
    "    secondary_term_outer = []\n",
    "    secondary_syn_outer = []\n",
    "    secondary_context_outer = []\n",
    "    \n",
    "    for i,j in metadata_df.iterrows():\n",
    "        \n",
    "        context = create_context(sentence_df, metadata_df, i)\n",
    "        \n",
    "        secondary_term_inner = [] \n",
    "        secondary_syn_inner = []\n",
    "        secondary_context_inner = None\n",
    "        term1 = term_dic[j['Metadata_Field']]\n",
    "        \n",
    "    \n",
    "        if len(term1) > 1:\n",
    "\n",
    "            for k in range(1,len(term1)):\n",
    "\n",
    "                item = str(term1[k])\n",
    "\n",
    "                syn = wordnet.synsets(item)\n",
    "\n",
    "                syn_list = [item]\n",
    "\n",
    "                #flatten all lists by chain, remove duplicates by set\n",
    "                lemmas = list(set(chain.from_iterable([w.lemma_names() for w in syn])))                \n",
    "\n",
    "                for m in lemmas[:3]:\n",
    "                    syn_list.append(m)\n",
    "\n",
    "                syn_list = list(set(syn_list))\n",
    "\n",
    "                for syn in syn_list:\n",
    "\n",
    "                    if syn in context.lower() and secondary_context_inner == None:\n",
    "                        secondary_term_inner.append(item)\n",
    "                        secondary_syn_inner.append(syn)\n",
    "                        secondary_context_inner = context\n",
    "\n",
    "                    elif syn in context.lower():\n",
    "                        secondary_term_inner.append(item)\n",
    "                        secondary_syn_inner.append(syn)\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "        else:\n",
    "            secondary_context_inner = context\n",
    "\n",
    "\n",
    "                                      \n",
    "        secondary_term_outer.append(secondary_term_inner)\n",
    "        secondary_syn_outer.append(secondary_syn_inner)\n",
    "        secondary_context_outer.append(secondary_context_inner)\n",
    "    \n",
    "    metadata_df['Secondary_Term'] = secondary_term_outer\n",
    "    metadata_df['Secondary_Synonym'] = secondary_syn_outer\n",
    "    metadata_df['Secondary_Context'] = secondary_context_outer\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs metadata dictionary\n",
    "#sentence found keyword dataframe\n",
    "#iterate through sentence dataframe\n",
    "# look at the combined terminology list of original_keyword and seondary_term\n",
    "# see if metadata search temrs are contained in the list\n",
    "# if all terms are in the sentence term list, then add the associated metadata term to the row as possible match\n",
    "\n",
    "def combine_keyword_list(df_keyword):\n",
    "    \n",
    "    complete_term_list = []\n",
    "    \n",
    "    for i,j in df_keyword.iterrows():\n",
    "        secondary_term_tolist = eval(str(j['Secondary_Term']))\n",
    "        original_keyword_string = str(j['Original_Keyword'])\n",
    "        \n",
    "        #print(type(original_keyword_string))\n",
    "        \n",
    "        #print(secondary_term_tolist)\n",
    "        secondary_term_tolist.append(original_keyword_string)\n",
    "        complete_terms = secondary_term_tolist\n",
    "        \n",
    "        complete_term_list.append(complete_terms)\n",
    "        \n",
    "    df_keyword['Complete_Term_List'] = complete_term_list\n",
    "    \n",
    "    return df_keyword\n",
    "\n",
    "##test\n",
    "#combine_keyword_list(df_sentence_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check each row to see if metadata fields are in the keywords list\n",
    "# create new column in dataframe that shows all possible metadata matches\n",
    "\n",
    "def metadata_match(df_keyword, dic_metadata, dic_original, dic_cat, dic_subcat):\n",
    "    \n",
    "    metadata_matches_outer = []\n",
    "    metadata_original_match_outer = []\n",
    "    metadata_category_outer = []\n",
    "    metadata_subcategory_outer = []\n",
    "    \n",
    "    \n",
    "    for i,j in df_keyword.iterrows():\n",
    "        \n",
    "        metadata_matches_inner = []\n",
    "        original_match_inner = []\n",
    "        category_match_inner = []\n",
    "        subcategory_match_inner = []\n",
    "\n",
    "                   \n",
    "        term_list = j['Complete_Term_List']\n",
    "        \n",
    "        for key in dic_metadata:\n",
    "            \n",
    "            if all(elem in term_list for elem in dic_metadata[key]):\n",
    "                metadata_matches_inner.append(key)\n",
    "                original_match_inner.append(dic_original[key][0])\n",
    "                category_match_inner.append(dic_cat[key][0])\n",
    "                subcategory_match_inner.append(dic_subcat[key][0])\n",
    "                                   \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        metadata_matches_outer.append(metadata_matches_inner)\n",
    "        metadata_original_match_outer.append(list(set(original_match_inner)))\n",
    "        metadata_category_outer.append(list(set(category_match_inner)))\n",
    "        metadata_subcategory_outer.append(list(set(subcategory_match_inner)))\n",
    "    \n",
    "    df_keyword['Metadata_Matches'] = metadata_matches_outer\n",
    "    df_keyword['Metadata_Original'] = metadata_original_match_outer\n",
    "    df_keyword['Metadata_Category'] = metadata_category_outer\n",
    "    df_keyword['Metadata_Subcategory'] = metadata_subcategory_outer\n",
    "    \n",
    "    #remove lines with know metadata field match\n",
    "    df_keyword = df_keyword[df_keyword['Metadata_Matches'].map(lambda d: len(d)) > 0]\n",
    "    \n",
    "    #print(df_keyword['Metadata_Matches'][1])\n",
    "    return df_keyword\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data and NER functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#takes full metadata excel file and grabs a few categories for display.\n",
    "#can add or remove columns as needed\n",
    "\n",
    "def clean_df(df_metadata):\n",
    "    df_metadata.head()\n",
    "    #grab relevant columns\n",
    "    df0 = df_metadata[['Document Name','Metadata_Category', 'Metadata_Original', 'Metadata_Matches', 'Secondary_Context']]\n",
    "    \n",
    "    \n",
    "    #rename for clarity\n",
    "    df0.columns = ['Document Name','Category', 'Original Metadata', 'Metadata Match', 'Context']\n",
    "    \n",
    "    \n",
    "    #categories = list(df0['Category'].unique())\n",
    "\n",
    "    df_clean = df0\n",
    "\n",
    "    for i,j in df_clean.iterrows():\n",
    "        metadata_cat_clean = str(j['Category']).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "        metadata_original_clean = str(j['Original Metadata']).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "        metadata_match_clean = str(j['Metadata Match']).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "\n",
    "\n",
    "        df_clean.at[i, 'Category'] = metadata_cat_clean\n",
    "        df_clean.at[i, 'Original Metadata'] = metadata_original_clean\n",
    "        df_clean.at[i, 'Metadata Match'] = metadata_match_clean\n",
    "        \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "#grab entities from context based on category\n",
    "def entity_extraction(df, entity_category_dic):\n",
    "    \n",
    "    #nlp1 = spacy.load('en_core_web_sm')\n",
    "    nlp1 = spacy.load('en_core_web_lg')\n",
    "    entity_list_outer = []\n",
    "\n",
    "    for i,j in df.iterrows():\n",
    "        \n",
    "        entity_list = []\n",
    "        \n",
    "        if j['Category'] in entity_category_dic:\n",
    "            \n",
    "            text1 = nlp1(j['Context'])\n",
    "            \n",
    "            for k in text1.ents:\n",
    "                if k.label_ in entity_category_dic[j['Category']]:\n",
    "                    entity_list.append(k.text)\n",
    "        \n",
    "        #identify uniques\n",
    "        entity_list = list(set(entity_list))\n",
    "        entity_list_outer.append(entity_list)\n",
    "      \n",
    "    df['Possible Matches'] = entity_list_outer   \n",
    "    \n",
    "    #df = df[df['Entities'].map(lambda d: len(d)) > 0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "Start with input dictionary containing category and keyword.  \n",
    "Use keyword to search synonyms. Create a tuple of (category, keyword, synonym) and append those to a list.  \n",
    "Iterate through list and search for synonym in text of df_sentence.  \n",
    "Create a dataframe of ['category', 'keyword', 'synonym', 'text']  \n",
    "\n",
    "\n",
    "Best algorithm for this? Right now looking at O(n_squared)\n",
    "    - (number of terms) * (number of sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FHLB_Chicago_2016_Renewal.pdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#create contract list\n",
    "contract_path = 'C:\\\\Users\\\\Hello\\\\Desktop\\\\metric_stream\\\\v0.024\\\\metadata_pipeline\\\\data\\\\'\n",
    "contract_names = listdir(contract_path)\n",
    "#remove .pdf Use for file name in csv export at end\n",
    "\n",
    "contract_files = [contract_path + f for f in listdir(contract_path) if isfile(join(contract_path, f))]\n",
    "file_name_list = [f for f in listdir(contract_path)]\n",
    "contract_number = 2\n",
    "\n",
    "file_name_list[contract_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can loop through list for contract files\n",
    "parsed_contract = parser.from_file(contract_files[contract_number])\n",
    "current_file_name = file_name_list[contract_number]\n",
    "\n",
    "pdf_doc = parsed_contract['content']\n",
    "\n",
    "\n",
    "#print(contract_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_meta_term is list of terminology associated with metadata field\n",
    "#dic_meta_category is mapping of metadata field to a metadata category (original metadata field provided to us)\n",
    "\n",
    "metadata_file = pd.read_excel('C:\\\\Users\\\\Hello\\\\Desktop\\\\metric_stream\\\\v0.024\\\\metadata_pipeline\\\\metadata_documents\\\\metadata_library.xlsx')\n",
    "dic_meta_term, dic_meta_original, dic_meta_cat, dic_meta_subcat = metadata_term_library(metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove newlines\n",
    "#Remove sentence that are too short (usually 1 or 2 symbols)\n",
    "\n",
    "sentence_list = sent_segment(pdf_doc)\n",
    "sentence_list_clean = []\n",
    "\n",
    "for i in range(0,len(sentence_list)):\n",
    "    remove_newline = sentence_list[i].replace('\\n', '')\n",
    "    if len(sentence_list[i]) > 4:\n",
    "        sentence_list_clean.append(remove_newline)\n",
    "    else: pass\n",
    "    \n",
    "#Write sentence list dataframe to file\n",
    "\n",
    "dic_sentence = {'Sentences' : sentence_list_clean} \n",
    "df_sentence = pd.DataFrame.from_dict(dic_sentence)\n",
    "\n",
    "#df_sentence.head()\n",
    "#df_sentence.to_csv('sentences0615.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match sentences with Metadata terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe containing Metadata field, original term1, and the synonym term1\n",
    "df_words = get_synonyms(dic_meta_term)\n",
    "\n",
    "\n",
    "df_metadata0 = search_term(df_sentence, df_words, file_name_list[contract_number])\n",
    "\n",
    "\n",
    "df_metadata_2 = search_secondary(df_sentence, df_metadata0, dic_meta_term)\n",
    "\n",
    "#df_metadata_2.to_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match found metadata terms to original metadata field and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_terms = combine_keyword_list(df_metadata_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAtch score (maybe)-> number of fields picked up (minimum 50% or something)\n",
    "\n",
    "df_metadata_match = metadata_match(df_combined_terms, dic_meta_term, dic_meta_original, dic_meta_cat, dic_meta_subcat)\n",
    "#df_metadata_match.head()\n",
    "#df_metadata_match.to_csv(f'results//{current_file_name}_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "PERSON\tPeople, including fictional.  \n",
    "NORP\tNationalities or religious or political groups.  \n",
    "FAC\tBuildings, airports, highways, bridges, etc.  \n",
    "ORG\tCompanies, agencies, institutions, etc.  \n",
    "GPE\tCountries, cities, states.  \n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water.  \n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.)  \n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc.  \n",
    "WORK_OF_ART\tTitles of books, songs, etc.  \n",
    "LAW\tNamed documents made into laws.  \n",
    "LANGUAGE\tAny named language.  \n",
    "DATE\tAbsolute or relative dates or periods.  \n",
    "TIME\tTimes smaller than a day.  \n",
    "PERCENT\tPercentage, including ”%“.  \n",
    "MONEY\tMonetary values, including unit.  \n",
    "QUANTITY\tMeasurements, as of weight or distance.  \n",
    "ORDINAL\t“first”, “second”, etc.  \n",
    "CARDINAL\tNumerals that do not fall under another type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#association dictionary of spacy NER to metadata category\n",
    "dic_category_entity = {'Fee': ['MONEY'], 'Date':['DATE'], 'Person': ['PERSON'], 'Product': ['PRODUCT', 'QUANTITY'], \\\n",
    "                       'Agreement': ['LAW'], 'Financial': ['MONEY'], 'Fee': ['MONEY'], 'Address': ['GPE'],\\\n",
    "                      'Legal': ['LAW']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hello\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#Run cleaner, entity extraction, and write to file\n",
    "df_clean = clean_df(df_metadata_match)\n",
    "\n",
    "df_ner_output = entity_extraction(df_clean, dic_category_entity)\n",
    "df_ner_output.to_csv(f'results//out_{file_name_list[contract_number]}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
